{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - From https://www.youtube.com/watch?v=kCc8FmEb1nY&t=3895s\n",
    " - Repo: https://github.com/karpathy/nanoGPT\n",
    " - google colab book: https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np     \n",
    "import pandas as pd    \n",
    "import matplotlib.pyplot as plt        \n",
    "import torch           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_mps = torch.backends.mps.is_built()\n",
    "has_mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/abhishekde/Desktop/Projects/DS/pytorch\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-28 11:20:59--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2024-11-28 11:20:59 (11.6 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"h\" in stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[c] for c in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"hii there\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hii there\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[: block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
      "when input is tensor([18]), target is 47\n",
      "when input is tensor([18, 47]), target is 56\n",
      "when input is tensor([18, 47, 56]), target is 57\n",
      "when input is tensor([18, 47, 56, 57]), target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]), target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]), target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]), target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1: block_size+1]\n",
    "print(train_data[:block_size+1])\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context}, target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[44,  1, 61, 47, 58, 46,  1, 63],\n",
      "        [27, 44,  1, 53, 59, 56,  1, 45],\n",
      "        [40, 59, 58,  1, 39,  1, 57, 50],\n",
      "        [ 1, 61, 46, 39, 58,  6,  1, 53]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 1, 61, 47, 58, 46,  1, 63, 53],\n",
      "        [44,  1, 53, 59, 56,  1, 45, 56],\n",
      "        [59, 58,  1, 39,  1, 57, 50, 47],\n",
      "        [61, 46, 39, 58,  6,  1, 53,  5]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1332)\n",
    "batch_size = 4 # how many independent sequences will be processed in parallel\n",
    "block_size = 8 # what is the maximum length of predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split==\"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1: i+1+block_size] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([44]), target is 1\n",
      "when input is tensor([44,  1]), target is 61\n",
      "when input is tensor([44,  1, 61]), target is 47\n",
      "when input is tensor([44,  1, 61, 47]), target is 58\n",
      "when input is tensor([44,  1, 61, 47, 58]), target is 46\n",
      "when input is tensor([44,  1, 61, 47, 58, 46]), target is 1\n",
      "when input is tensor([44,  1, 61, 47, 58, 46,  1]), target is 63\n",
      "when input is tensor([44,  1, 61, 47, 58, 46,  1, 63]), target is 53\n",
      "when input is tensor([27]), target is 44\n",
      "when input is tensor([27, 44]), target is 1\n",
      "when input is tensor([27, 44,  1]), target is 53\n",
      "when input is tensor([27, 44,  1, 53]), target is 59\n",
      "when input is tensor([27, 44,  1, 53, 59]), target is 56\n",
      "when input is tensor([27, 44,  1, 53, 59, 56]), target is 1\n",
      "when input is tensor([27, 44,  1, 53, 59, 56,  1]), target is 45\n",
      "when input is tensor([27, 44,  1, 53, 59, 56,  1, 45]), target is 56\n",
      "when input is tensor([40]), target is 59\n",
      "when input is tensor([40, 59]), target is 58\n",
      "when input is tensor([40, 59, 58]), target is 1\n",
      "when input is tensor([40, 59, 58,  1]), target is 39\n",
      "when input is tensor([40, 59, 58,  1, 39]), target is 1\n",
      "when input is tensor([40, 59, 58,  1, 39,  1]), target is 57\n",
      "when input is tensor([40, 59, 58,  1, 39,  1, 57]), target is 50\n",
      "when input is tensor([40, 59, 58,  1, 39,  1, 57, 50]), target is 47\n",
      "when input is tensor([1]), target is 61\n",
      "when input is tensor([ 1, 61]), target is 46\n",
      "when input is tensor([ 1, 61, 46]), target is 39\n",
      "when input is tensor([ 1, 61, 46, 39]), target is 58\n",
      "when input is tensor([ 1, 61, 46, 39, 58]), target is 6\n",
      "when input is tensor([ 1, 61, 46, 39, 58,  6]), target is 1\n",
      "when input is tensor([ 1, 61, 46, 39, 58,  6,  1]), target is 53\n",
      "when input is tensor([ 1, 61, 46, 39, 58,  6,  1, 53]), target is 5\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context}, target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9c7a0ae1b0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn   \n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1332)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_embeddings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0membedding_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpadding_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_norm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnorm_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msparse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0m_weight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0m_freeze\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
      "\n",
      "This module is often used to store word embeddings and retrieve them using indices.\n",
      "The input to the module is a list of indices, and the output is the corresponding\n",
      "word embeddings.\n",
      "\n",
      "Args:\n",
      "    num_embeddings (int): size of the dictionary of embeddings\n",
      "    embedding_dim (int): the size of each embedding vector\n",
      "    padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;\n",
      "                                 therefore, the embedding vector at :attr:`padding_idx` is not updated during training,\n",
      "                                 i.e. it remains as a fixed \"pad\". For a newly constructed Embedding,\n",
      "                                 the embedding vector at :attr:`padding_idx` will default to all zeros,\n",
      "                                 but can be updated to another value to be used as the padding vector.\n",
      "    max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`\n",
      "                                is renormalized to have norm :attr:`max_norm`.\n",
      "    norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.\n",
      "    scale_grad_by_freq (bool, optional): If given, this will scale gradients by the inverse of frequency of\n",
      "                                            the words in the mini-batch. Default ``False``.\n",
      "    sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` matrix will be a sparse tensor.\n",
      "                             See Notes for more details regarding sparse gradients.\n",
      "\n",
      "Attributes:\n",
      "    weight (Tensor): the learnable weights of the module of shape (num_embeddings, embedding_dim)\n",
      "                     initialized from :math:`\\mathcal{N}(0, 1)`\n",
      "\n",
      "Shape:\n",
      "    - Input: :math:`(*)`, IntTensor or LongTensor of arbitrary shape containing the indices to extract\n",
      "    - Output: :math:`(*, H)`, where `*` is the input shape and :math:`H=\\text{embedding\\_dim}`\n",
      "\n",
      ".. note::\n",
      "    Keep in mind that only a limited number of optimizers support\n",
      "    sparse gradients: currently it's :class:`optim.SGD` (`CUDA` and `CPU`),\n",
      "    :class:`optim.SparseAdam` (`CUDA` and `CPU`) and :class:`optim.Adagrad` (`CPU`)\n",
      "\n",
      ".. note::\n",
      "    When :attr:`max_norm` is not ``None``, :class:`Embedding`'s forward method will modify the\n",
      "    :attr:`weight` tensor in-place. Since tensors needed for gradient computations cannot be\n",
      "    modified in-place, performing a differentiable operation on ``Embedding.weight`` before\n",
      "    calling :class:`Embedding`'s forward method requires cloning ``Embedding.weight`` when\n",
      "    :attr:`max_norm` is not ``None``. For example::\n",
      "\n",
      "        n, d, m = 3, 5, 7\n",
      "        embedding = nn.Embedding(n, d, max_norm=True)\n",
      "        W = torch.randn((m, d), requires_grad=True)\n",
      "        idx = torch.tensor([1, 2])\n",
      "        a = embedding.weight.clone() @ W.t()  # weight must be cloned for this to be differentiable\n",
      "        b = embedding(idx) @ W.t()  # modifies weight in-place\n",
      "        out = (a.unsqueeze(0) + b.unsqueeze(1))\n",
      "        loss = out.sigmoid().prod()\n",
      "        loss.backward()\n",
      "\n",
      "Examples::\n",
      "\n",
      "    >>> # an Embedding module containing 10 tensors of size 3\n",
      "    >>> embedding = nn.Embedding(10, 3)\n",
      "    >>> # a batch of 2 samples of 4 indices each\n",
      "    >>> input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n",
      "    >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "    >>> embedding(input)\n",
      "    tensor([[[-0.0251, -1.6902,  0.7172],\n",
      "             [-0.6431,  0.0748,  0.6969],\n",
      "             [ 1.4970,  1.3448, -0.9685],\n",
      "             [-0.3677, -2.7265, -0.1685]],\n",
      "\n",
      "            [[ 1.4970,  1.3448, -0.9685],\n",
      "             [ 0.4362, -0.4004,  0.9400],\n",
      "             [-0.6431,  0.0748,  0.6969],\n",
      "             [ 0.9124, -2.3616,  1.1151]]])\n",
      "\n",
      "\n",
      "    >>> # example with padding_idx\n",
      "    >>> embedding = nn.Embedding(10, 3, padding_idx=0)\n",
      "    >>> input = torch.LongTensor([[0, 2, 0, 5]])\n",
      "    >>> embedding(input)\n",
      "    tensor([[[ 0.0000,  0.0000,  0.0000],\n",
      "             [ 0.1535, -2.0309,  0.9315],\n",
      "             [ 0.0000,  0.0000,  0.0000],\n",
      "             [-0.1655,  0.9897,  0.0635]]])\n",
      "\n",
      "    >>> # example of changing `pad` vector\n",
      "    >>> padding_idx = 0\n",
      "    >>> embedding = nn.Embedding(3, 3, padding_idx=padding_idx)\n",
      "    >>> embedding.weight\n",
      "    Parameter containing:\n",
      "    tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "            [-0.7895, -0.7089, -0.0364],\n",
      "            [ 0.6778,  0.5803,  0.2678]], requires_grad=True)\n",
      "    >>> with torch.no_grad():\n",
      "    ...     embedding.weight[padding_idx] = torch.ones(3)\n",
      "    >>> embedding.weight\n",
      "    Parameter containing:\n",
      "    tensor([[ 1.0000,  1.0000,  1.0000],\n",
      "            [-0.7895, -0.7089, -0.0364],\n",
      "            [ 0.6778,  0.5803,  0.2678]], requires_grad=True)\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/Desktop/Projects/.venv/lib/python3.8/site-packages/torch/nn/modules/sparse.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     Embedding, Embedding"
     ]
    }
   ],
   "source": [
    "nn.Embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(10, 10)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em = nn.Embedding(10, 10)\n",
    "em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__constants__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_fill_padding_idx_with_zero',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'embedding_dim',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'from_pretrained',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'load_state_dict',\n",
       " 'max_norm',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'norm_type',\n",
       " 'num_embeddings',\n",
       " 'padding_idx',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'reset_parameters',\n",
       " 'scale_grad_by_freq',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'sparse',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'weight',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.0175, -0.2013,  0.7380,  0.6510, -0.0429, -0.4522, -1.5231,  0.3527,\n",
       "          0.3391,  0.5226],\n",
       "        [-1.4219, -0.0108, -0.3183,  1.1943, -1.1718, -0.1582, -1.7327, -0.5154,\n",
       "         -2.0280, -2.1273],\n",
       "        [-0.0853, -0.3235,  0.3193, -0.6140, -1.3470,  0.3112,  0.2305,  0.2442,\n",
       "         -0.1316, -1.1528],\n",
       "        [ 1.0391, -1.7309,  1.6593,  0.0595, -0.2043, -0.8348, -1.3504,  1.7978,\n",
       "          0.2779,  0.0637],\n",
       "        [-1.8127,  0.2159, -0.3877,  1.3709, -0.4423,  0.5503,  1.1506,  0.9796,\n",
       "          0.8442, -0.0377],\n",
       "        [-0.7907,  1.4548, -1.2190,  0.5885,  0.0604,  1.7116,  0.9181,  0.2841,\n",
       "         -1.2448,  0.5812],\n",
       "        [ 0.4969, -0.4833,  0.9447, -0.4740, -1.5136, -1.4680,  0.5735, -0.0234,\n",
       "          0.4481,  0.5951],\n",
       "        [ 0.7591, -0.4095,  0.1171, -1.0498, -0.4200, -1.5601, -0.9316, -1.5470,\n",
       "         -0.0929,  2.1709],\n",
       "        [ 1.0895,  1.9371,  0.0048,  0.5337, -1.1086,  0.2672,  0.6059,  0.1532,\n",
       "         -1.0170,  0.1772],\n",
       "        [ 0.3271,  0.1050, -0.4085, -0.4086, -0.6023,  0.2354, -0.0113,  1.0268,\n",
       "          0.0196, -0.4074]], requires_grad=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "multinomial(input, num_samples, replacement=False, *, generator=None, out=None) -> LongTensor\n",
      "\n",
      "Returns a tensor where each row contains :attr:`num_samples` indices sampled\n",
      "from the multinomial (a stricter definition would be multivariate,\n",
      "refer to torch.distributions.multinomial.Multinomial for more details)\n",
      "probability distribution located in the corresponding row\n",
      "of tensor :attr:`input`.\n",
      "\n",
      ".. note::\n",
      "    The rows of :attr:`input` do not need to sum to one (in which case we use\n",
      "    the values as weights), but must be non-negative, finite and have\n",
      "    a non-zero sum.\n",
      "\n",
      "Indices are ordered from left to right according to when each was sampled\n",
      "(first samples are placed in first column).\n",
      "\n",
      "If :attr:`input` is a vector, :attr:`out` is a vector of size :attr:`num_samples`.\n",
      "\n",
      "If :attr:`input` is a matrix with `m` rows, :attr:`out` is an matrix of shape\n",
      ":math:`(m \\times \\text{num\\_samples})`.\n",
      "\n",
      "If replacement is ``True``, samples are drawn with replacement.\n",
      "\n",
      "If not, they are drawn without replacement, which means that when a\n",
      "sample index is drawn for a row, it cannot be drawn again for that row.\n",
      "\n",
      ".. note::\n",
      "    When drawn without replacement, :attr:`num_samples` must be lower than\n",
      "    number of non-zero elements in :attr:`input` (or the min number of non-zero\n",
      "    elements in each row of :attr:`input` if it is a matrix).\n",
      "\n",
      "Args:\n",
      "    input (Tensor): the input tensor containing probabilities\n",
      "    num_samples (int): number of samples to draw\n",
      "    replacement (bool, optional): whether to draw with replacement or not\n",
      "\n",
      "Keyword args:\n",
      "    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n",
      "    out (Tensor, optional): the output tensor.\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> weights = torch.tensor([0, 10, 3, 0], dtype=torch.float) # create a tensor of weights\n",
      "    >>> torch.multinomial(weights, 2)\n",
      "    tensor([1, 2])\n",
      "    >>> torch.multinomial(weights, 4) # ERROR!\n",
      "    RuntimeError: invalid argument 2: invalid multinomial distribution (with replacement=False,\n",
      "    not enough non-negative category to sample) at ../aten/src/TH/generic/THTensorRandom.cpp:320\n",
      "    >>> torch.multinomial(weights, 4, replacement=True)\n",
      "    tensor([ 2,  1,  1,  1])\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "torch.multinomial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 41\u001b[0m\n\u001b[1;32m     37\u001b[0m             idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((idx, idx_next), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m idx\n\u001b[0;32m---> 41\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mBiGramLanguageModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m m(xb, yb)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "## BiGram language model\n",
    "n_embd = 32 \n",
    "class BiGramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else: \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "      \n",
    "    \n",
    "m = BiGramLanguageModel()\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nVm-Yb;oU3gcqLiBN-&YDOlmlTDJAohXTR$NDXuB,WJpA\\ny?!kMnABVPmQlWgHB3,oc'qCbvlnyM:TZkPopJnG&vrCoIrs&-Skt Z\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "decode(m.generate(idx, max_new_tokens = 100)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pytorch optimizer object\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.423017978668213\n"
     ]
    }
   ],
   "source": [
    "# Train the BiGram model\n",
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    \n",
    "    # evaluate loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "print(loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHigamee ut ares atru poo s l-ieilouney halavire t on blis.\\nFII' cughio tho y ners? If wesith \\nPrs d gindu ave thile monghe in mimbivefro m merathand I y k's an y. A:\\nGulursouitwolthe w ARIG macore hicieazererk y ice r ipe to pass apthis, kivee urkn Who wnd the? tovepe s hillindorweovistomary LAnenoviof aren tond fos athe hat my more, y 'd, y winf\\n\\nDor s putheye ale'sereacat IUKE:'d ithyowsblly l!\\n\\nHAnd and hethy, t mirs ajerantas Buime o mseditithall alou s,\\nFe j; s IOLidin ick hmy y\\nI'Rilyoll:\\nAR: tcerare ave; lon, qu tlicla ss h, sorgs acaroves s pllos f mave vaslase one fingo w, sofeanthe, ith icintharind y thed ghan ICHathes HAn s heous ajove ind be 'd,\\nY:\\nAUThowein: 'sppres fe h intout\\nMy lled s:\\nIO'sene or'st he.\\nBuchu t t bove mbr non\\nINClouleerf ch mase apserr revetothinerdothowe d nthe wofod nqu ETRCARS:\\nWebrunlliserat,\\nRENG ppthe?\\nAle the playondo fally blom ryouthe ts:\\nTes m w le'lendondyore prsumso isagaspevantsasiove oulller\\nAR:\\nTUSTCEWelplaryord g ame t hiss fthord apre. \""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "decode(m.generate(idx, max_new_tokens = 1000)[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1332)\n",
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "x = torch.rand(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(x_bow, x_bow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2151, 0.3679],\n",
       "         [0.7507, 0.9052]],\n",
       "\n",
       "        [[0.3940, 0.9941],\n",
       "         [0.4775, 0.9544]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:2, :2, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2151, 0.3679],\n",
       "        [0.7507, 0.9052],\n",
       "        [0.2884, 0.7917],\n",
       "        [0.2249, 0.1734],\n",
       "        [0.0311, 0.2682],\n",
       "        [0.9956, 0.7544],\n",
       "        [0.3207, 0.2828],\n",
       "        [0.6517, 0.0474]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1\n",
    "x_bow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]\n",
    "        x_bow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2151, 0.3679],\n",
       "        [0.4829, 0.6366],\n",
       "        [0.4180, 0.6883],\n",
       "        [0.3697, 0.5596],\n",
       "        [0.3020, 0.5013],\n",
       "        [0.4176, 0.5435],\n",
       "        [0.4038, 0.5062],\n",
       "        [0.4348, 0.4489]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones((3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2151, 0.3679],\n",
       "        [0.4829, 0.6366],\n",
       "        [0.4180, 0.6883],\n",
       "        [0.3697, 0.5596],\n",
       "        [0.3020, 0.5013],\n",
       "        [0.4176, 0.5435],\n",
       "        [0.4038, 0.5062],\n",
       "        [0.4348, 0.4489]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2\n",
    "wei = torch.tril(torch.ones((T,T)))\n",
    "wei = wei/torch.sum(wei, 1, keepdim=True) \n",
    "x_bow2 = wei @ x # (B, T, T) @ (B, T, C)  -> (B, T, C)\n",
    "x_bow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=tensor([[5., 3.],\n",
      "        [3., 4.],\n",
      "        [4., 6.]])\n",
      "c=tensor([[5.0000, 3.0000],\n",
      "        [4.0000, 3.5000],\n",
      "        [4.0000, 4.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(32)\n",
    "a = torch.tril(torch.ones((3,3)))\n",
    "a = a/torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "print(f\"a={a}\")\n",
    "print(f\"b={b}\")\n",
    "print(f\"c={c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.0000, 3.0000],\n",
       "        [4.0000, 3.5000],\n",
       "        [4.0000, 4.3333]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(a, b) # same as a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 32])\n"
     ]
    }
   ],
   "source": [
    "# version 4: self attention\n",
    "torch.manual_seed(32)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "out = wei @ x\n",
    "print(out.shape)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading",
   "language": "python",
   "name": "trading"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
