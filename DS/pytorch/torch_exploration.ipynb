{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Torch exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d =torch.zeros((5,5))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m      <no docstring>\n",
      "\u001b[0;31mFile:\u001b[0m           ~/Desktop/Projects/.venv/lib/python3.8/site-packages/torch/__init__.py\n",
      "\u001b[0;31mType:\u001b[0m           _TensorMeta\n",
      "\u001b[0;31mSubclasses:\u001b[0m     SparseSemiStructuredTensor, Parameter, UninitializedBuffer, MaskedTensor, FakeTensor, FunctionalTensor"
     ]
    }
   ],
   "source": [
    "torch.Tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3.], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.Tensor([1,2,3])\n",
    "m.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "tensor(data, *, dtype=None, device=None, requires_grad=False, pin_memory=False) -> Tensor\n",
      "\n",
      "Constructs a tensor with no autograd history (also known as a \"leaf tensor\", see :doc:`/notes/autograd`) by copying :attr:`data`.\n",
      "\n",
      ".. warning::\n",
      "\n",
      "    When working with tensors prefer using :func:`torch.Tensor.clone`,\n",
      "    :func:`torch.Tensor.detach`, and :func:`torch.Tensor.requires_grad_` for\n",
      "    readability. Letting `t` be a tensor, ``torch.tensor(t)`` is equivalent to\n",
      "    ``t.clone().detach()``, and ``torch.tensor(t, requires_grad=True)``\n",
      "    is equivalent to ``t.clone().detach().requires_grad_(True)``.\n",
      "\n",
      ".. seealso::\n",
      "\n",
      "    :func:`torch.as_tensor` preserves autograd history and avoids copies where possible.\n",
      "    :func:`torch.from_numpy` creates a tensor that shares storage with a NumPy array.\n",
      "\n",
      "Args:\n",
      "    data (array_like): Initial data for the tensor. Can be a list, tuple,\n",
      "        NumPy ``ndarray``, scalar, and other types.\n",
      "\n",
      "Keyword args:\n",
      "    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "        Default: if ``None``, infers data type from :attr:`data`.\n",
      "    device (:class:`torch.device`, optional): the device of the constructed tensor. If None and data is a tensor\n",
      "        then the device of data is used. If None and data is not a tensor then\n",
      "        the result tensor is constructed on the current device.\n",
      "    requires_grad (bool, optional): If autograd should record operations on the\n",
      "        returned tensor. Default: ``False``.\n",
      "    pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      "        the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      "\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\n",
      "    tensor([[ 0.1000,  1.2000],\n",
      "            [ 2.2000,  3.1000],\n",
      "            [ 4.9000,  5.2000]])\n",
      "\n",
      "    >>> torch.tensor([0, 1])  # Type inference on data\n",
      "    tensor([ 0,  1])\n",
      "\n",
      "    >>> torch.tensor([[0.11111, 0.222222, 0.3333333]],\n",
      "    ...              dtype=torch.float64,\n",
      "    ...              device=torch.device('cuda:0'))  # creates a double tensor on a CUDA device\n",
      "    tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')\n",
      "\n",
      "    >>> torch.tensor(3.14159)  # Create a zero-dimensional (scalar) tensor\n",
      "    tensor(3.1416)\n",
      "\n",
      "    >>> torch.tensor([])  # Create an empty tensor (of size (0,))\n",
      "    tensor([])\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "torch.tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m        module\n",
      "\u001b[0;31mString form:\u001b[0m <module 'torch.nn' from '/Users/abhishekde/Desktop/Projects/.venv/lib/python3.8/site-packages/torch/nn/__init__.py'>\n",
      "\u001b[0;31mFile:\u001b[0m        ~/Desktop/Projects/.venv/lib/python3.8/site-packages/torch/nn/__init__.py\n",
      "\u001b[0;31mDocstring:\u001b[0m   <no docstring>"
     ]
    }
   ],
   "source": [
    "nn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0min_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mout_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.\n",
      "\n",
      "This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "\n",
      "On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "\n",
      "Args:\n",
      "    in_features: size of each input sample\n",
      "    out_features: size of each output sample\n",
      "    bias: If set to ``False``, the layer will not learn an additive bias.\n",
      "        Default: ``True``\n",
      "\n",
      "Shape:\n",
      "    - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n",
      "      dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n",
      "    - Output: :math:`(*, H_{out})` where all but the last dimension\n",
      "      are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
      "\n",
      "Attributes:\n",
      "    weight: the learnable weights of the module of shape\n",
      "        :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
      "        initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
      "        :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "    bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
      "            If :attr:`bias` is ``True``, the values are initialized from\n",
      "            :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "\n",
      "Examples::\n",
      "\n",
      "    >>> m = nn.Linear(20, 30)\n",
      "    >>> input = torch.randn(128, 20)\n",
      "    >>> output = m(input)\n",
      "    >>> print(output.size())\n",
      "    torch.Size([128, 30])\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/Desktop/Projects/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     NonDynamicallyQuantizableLinear, LazyLinear, Linear, LinearBn1d, Linear"
     ]
    }
   ],
   "source": [
    "nn.Linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_layer = nn.Linear(in_features = 5, out_features=1, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Base class for all neural network modules.\n",
      "\n",
      "Your models should also subclass this class.\n",
      "\n",
      "Modules can also contain other Modules, allowing to nest them in\n",
      "a tree structure. You can assign the submodules as regular attributes::\n",
      "\n",
      "    import torch.nn as nn\n",
      "    import torch.nn.functional as F\n",
      "\n",
      "    class Model(nn.Module):\n",
      "        def __init__(self):\n",
      "            super().__init__()\n",
      "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
      "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
      "\n",
      "        def forward(self, x):\n",
      "            x = F.relu(self.conv1(x))\n",
      "            return F.relu(self.conv2(x))\n",
      "\n",
      "Submodules assigned in this way will be registered, and will have their\n",
      "parameters converted too when you call :meth:`to`, etc.\n",
      "\n",
      ".. note::\n",
      "    As per the example above, an ``__init__()`` call to the parent class\n",
      "    must be made before assignment on the child.\n",
      "\n",
      ":ivar training: Boolean represents whether this module is in training or\n",
      "                evaluation mode.\n",
      ":vartype training: bool\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/Desktop/Projects/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     Identity, Linear, Bilinear, _ConvNd, Threshold, ReLU, RReLU, Hardtanh, Sigmoid, Hardsigmoid, ..."
     ]
    }
   ],
   "source": [
    "nn.Module?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # initialize the base class\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return F.relu(self.conv2(x))\n",
    "    \n",
    "dummy_model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AdaptiveAvgPool1d',\n",
       " 'AdaptiveAvgPool2d',\n",
       " 'AdaptiveAvgPool3d',\n",
       " 'AdaptiveLogSoftmaxWithLoss',\n",
       " 'AdaptiveMaxPool1d',\n",
       " 'AdaptiveMaxPool2d',\n",
       " 'AdaptiveMaxPool3d',\n",
       " 'AlphaDropout',\n",
       " 'AvgPool1d',\n",
       " 'AvgPool2d',\n",
       " 'AvgPool3d',\n",
       " 'BCELoss',\n",
       " 'BCEWithLogitsLoss',\n",
       " 'BatchNorm1d',\n",
       " 'BatchNorm2d',\n",
       " 'BatchNorm3d',\n",
       " 'Bilinear',\n",
       " 'CELU',\n",
       " 'CTCLoss',\n",
       " 'ChannelShuffle',\n",
       " 'CircularPad1d',\n",
       " 'CircularPad2d',\n",
       " 'CircularPad3d',\n",
       " 'ConstantPad1d',\n",
       " 'ConstantPad2d',\n",
       " 'ConstantPad3d',\n",
       " 'Container',\n",
       " 'Conv1d',\n",
       " 'Conv2d',\n",
       " 'Conv3d',\n",
       " 'ConvTranspose1d',\n",
       " 'ConvTranspose2d',\n",
       " 'ConvTranspose3d',\n",
       " 'CosineEmbeddingLoss',\n",
       " 'CosineSimilarity',\n",
       " 'CrossEntropyLoss',\n",
       " 'CrossMapLRN2d',\n",
       " 'DataParallel',\n",
       " 'Dropout',\n",
       " 'Dropout1d',\n",
       " 'Dropout2d',\n",
       " 'Dropout3d',\n",
       " 'ELU',\n",
       " 'Embedding',\n",
       " 'EmbeddingBag',\n",
       " 'FeatureAlphaDropout',\n",
       " 'Flatten',\n",
       " 'Fold',\n",
       " 'FractionalMaxPool2d',\n",
       " 'FractionalMaxPool3d',\n",
       " 'GELU',\n",
       " 'GLU',\n",
       " 'GRU',\n",
       " 'GRUCell',\n",
       " 'GaussianNLLLoss',\n",
       " 'GroupNorm',\n",
       " 'Hardshrink',\n",
       " 'Hardsigmoid',\n",
       " 'Hardswish',\n",
       " 'Hardtanh',\n",
       " 'HingeEmbeddingLoss',\n",
       " 'HuberLoss',\n",
       " 'Identity',\n",
       " 'InstanceNorm1d',\n",
       " 'InstanceNorm2d',\n",
       " 'InstanceNorm3d',\n",
       " 'KLDivLoss',\n",
       " 'L1Loss',\n",
       " 'LPPool1d',\n",
       " 'LPPool2d',\n",
       " 'LSTM',\n",
       " 'LSTMCell',\n",
       " 'LayerNorm',\n",
       " 'LazyBatchNorm1d',\n",
       " 'LazyBatchNorm2d',\n",
       " 'LazyBatchNorm3d',\n",
       " 'LazyConv1d',\n",
       " 'LazyConv2d',\n",
       " 'LazyConv3d',\n",
       " 'LazyConvTranspose1d',\n",
       " 'LazyConvTranspose2d',\n",
       " 'LazyConvTranspose3d',\n",
       " 'LazyInstanceNorm1d',\n",
       " 'LazyInstanceNorm2d',\n",
       " 'LazyInstanceNorm3d',\n",
       " 'LazyLinear',\n",
       " 'LeakyReLU',\n",
       " 'Linear',\n",
       " 'LocalResponseNorm',\n",
       " 'LogSigmoid',\n",
       " 'LogSoftmax',\n",
       " 'MSELoss',\n",
       " 'MarginRankingLoss',\n",
       " 'MaxPool1d',\n",
       " 'MaxPool2d',\n",
       " 'MaxPool3d',\n",
       " 'MaxUnpool1d',\n",
       " 'MaxUnpool2d',\n",
       " 'MaxUnpool3d',\n",
       " 'Mish',\n",
       " 'Module',\n",
       " 'ModuleDict',\n",
       " 'ModuleList',\n",
       " 'MultiLabelMarginLoss',\n",
       " 'MultiLabelSoftMarginLoss',\n",
       " 'MultiMarginLoss',\n",
       " 'MultiheadAttention',\n",
       " 'NLLLoss',\n",
       " 'NLLLoss2d',\n",
       " 'PReLU',\n",
       " 'PairwiseDistance',\n",
       " 'Parameter',\n",
       " 'ParameterDict',\n",
       " 'ParameterList',\n",
       " 'PixelShuffle',\n",
       " 'PixelUnshuffle',\n",
       " 'PoissonNLLLoss',\n",
       " 'RNN',\n",
       " 'RNNBase',\n",
       " 'RNNCell',\n",
       " 'RNNCellBase',\n",
       " 'RReLU',\n",
       " 'ReLU',\n",
       " 'ReLU6',\n",
       " 'ReflectionPad1d',\n",
       " 'ReflectionPad2d',\n",
       " 'ReflectionPad3d',\n",
       " 'ReplicationPad1d',\n",
       " 'ReplicationPad2d',\n",
       " 'ReplicationPad3d',\n",
       " 'SELU',\n",
       " 'Sequential',\n",
       " 'SiLU',\n",
       " 'Sigmoid',\n",
       " 'SmoothL1Loss',\n",
       " 'SoftMarginLoss',\n",
       " 'Softmax',\n",
       " 'Softmax2d',\n",
       " 'Softmin',\n",
       " 'Softplus',\n",
       " 'Softshrink',\n",
       " 'Softsign',\n",
       " 'SyncBatchNorm',\n",
       " 'Tanh',\n",
       " 'Tanhshrink',\n",
       " 'Threshold',\n",
       " 'Transformer',\n",
       " 'TransformerDecoder',\n",
       " 'TransformerDecoderLayer',\n",
       " 'TransformerEncoder',\n",
       " 'TransformerEncoderLayer',\n",
       " 'TripletMarginLoss',\n",
       " 'TripletMarginWithDistanceLoss',\n",
       " 'Unflatten',\n",
       " 'Unfold',\n",
       " 'UninitializedBuffer',\n",
       " 'UninitializedParameter',\n",
       " 'Upsample',\n",
       " 'UpsamplingBilinear2d',\n",
       " 'UpsamplingNearest2d',\n",
       " 'ZeroPad1d',\n",
       " 'ZeroPad2d',\n",
       " 'ZeroPad3d',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_reduction',\n",
       " 'common_types',\n",
       " 'factory_kwargs',\n",
       " 'functional',\n",
       " 'grad',\n",
       " 'init',\n",
       " 'intrinsic',\n",
       " 'modules',\n",
       " 'parallel',\n",
       " 'parameter',\n",
       " 'qat',\n",
       " 'quantizable',\n",
       " 'quantized',\n",
       " 'utils']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BroadcastingList1',\n",
       " 'BroadcastingList2',\n",
       " 'BroadcastingList3',\n",
       " 'Callable',\n",
       " 'DType',\n",
       " 'GRID_SAMPLE_INTERPOLATION_MODES',\n",
       " 'GRID_SAMPLE_PADDING_MODES',\n",
       " 'List',\n",
       " 'Optional',\n",
       " 'TYPE_CHECKING',\n",
       " 'Tensor',\n",
       " 'Tuple',\n",
       " 'Union',\n",
       " '_Reduction',\n",
       " '_VF',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_adaptive_max_pool1d',\n",
       " '_adaptive_max_pool2d',\n",
       " '_adaptive_max_pool3d',\n",
       " '_add_docstr',\n",
       " '_canonical_mask',\n",
       " '_fractional_max_pool2d',\n",
       " '_fractional_max_pool3d',\n",
       " '_get_softmax_dim',\n",
       " '_in_projection',\n",
       " '_in_projection_packed',\n",
       " '_infer_size',\n",
       " '_is_integer',\n",
       " '_list_with_default',\n",
       " '_max_pool1d',\n",
       " '_max_pool2d',\n",
       " '_max_pool3d',\n",
       " '_mha_shape_check',\n",
       " '_no_grad_embedding_renorm_',\n",
       " '_none_or_dtype',\n",
       " '_overload',\n",
       " '_pair',\n",
       " '_single',\n",
       " '_sym_int',\n",
       " '_threshold',\n",
       " '_triple',\n",
       " '_unpool_output_size',\n",
       " '_verify_batch_size',\n",
       " '_verify_spatial_size',\n",
       " 'adaptive_avg_pool1d',\n",
       " 'adaptive_avg_pool2d',\n",
       " 'adaptive_avg_pool3d',\n",
       " 'adaptive_max_pool1d',\n",
       " 'adaptive_max_pool1d_with_indices',\n",
       " 'adaptive_max_pool2d',\n",
       " 'adaptive_max_pool2d_with_indices',\n",
       " 'adaptive_max_pool3d',\n",
       " 'adaptive_max_pool3d_with_indices',\n",
       " 'affine_grid',\n",
       " 'alpha_dropout',\n",
       " 'assert_int_or_pair',\n",
       " 'avg_pool1d',\n",
       " 'avg_pool2d',\n",
       " 'avg_pool3d',\n",
       " 'batch_norm',\n",
       " 'bilinear',\n",
       " 'binary_cross_entropy',\n",
       " 'binary_cross_entropy_with_logits',\n",
       " 'boolean_dispatch',\n",
       " 'celu',\n",
       " 'celu_',\n",
       " 'channel_shuffle',\n",
       " 'conv1d',\n",
       " 'conv2d',\n",
       " 'conv3d',\n",
       " 'conv_tbc',\n",
       " 'conv_transpose1d',\n",
       " 'conv_transpose2d',\n",
       " 'conv_transpose3d',\n",
       " 'cosine_embedding_loss',\n",
       " 'cosine_similarity',\n",
       " 'cross_entropy',\n",
       " 'ctc_loss',\n",
       " 'dropout',\n",
       " 'dropout1d',\n",
       " 'dropout2d',\n",
       " 'dropout3d',\n",
       " 'elu',\n",
       " 'elu_',\n",
       " 'embedding',\n",
       " 'embedding_bag',\n",
       " 'feature_alpha_dropout',\n",
       " 'fold',\n",
       " 'fractional_max_pool2d',\n",
       " 'fractional_max_pool2d_with_indices',\n",
       " 'fractional_max_pool3d',\n",
       " 'fractional_max_pool3d_with_indices',\n",
       " 'gaussian_nll_loss',\n",
       " 'gelu',\n",
       " 'glu',\n",
       " 'grad',\n",
       " 'grid_sample',\n",
       " 'group_norm',\n",
       " 'gumbel_softmax',\n",
       " 'handle_torch_function',\n",
       " 'hardshrink',\n",
       " 'hardsigmoid',\n",
       " 'hardswish',\n",
       " 'hardtanh',\n",
       " 'hardtanh_',\n",
       " 'has_torch_function',\n",
       " 'has_torch_function_unary',\n",
       " 'has_torch_function_variadic',\n",
       " 'hinge_embedding_loss',\n",
       " 'huber_loss',\n",
       " 'importlib',\n",
       " 'instance_norm',\n",
       " 'interpolate',\n",
       " 'kl_div',\n",
       " 'l1_loss',\n",
       " 'layer_norm',\n",
       " 'leaky_relu',\n",
       " 'leaky_relu_',\n",
       " 'linear',\n",
       " 'local_response_norm',\n",
       " 'log_softmax',\n",
       " 'logsigmoid',\n",
       " 'lp_pool1d',\n",
       " 'lp_pool2d',\n",
       " 'margin_ranking_loss',\n",
       " 'math',\n",
       " 'max_pool1d',\n",
       " 'max_pool1d_with_indices',\n",
       " 'max_pool2d',\n",
       " 'max_pool2d_with_indices',\n",
       " 'max_pool3d',\n",
       " 'max_pool3d_with_indices',\n",
       " 'max_unpool1d',\n",
       " 'max_unpool2d',\n",
       " 'max_unpool3d',\n",
       " 'mish',\n",
       " 'mse_loss',\n",
       " 'multi_head_attention_forward',\n",
       " 'multi_margin_loss',\n",
       " 'multilabel_margin_loss',\n",
       " 'multilabel_soft_margin_loss',\n",
       " 'native_channel_shuffle',\n",
       " 'nll_loss',\n",
       " 'normalize',\n",
       " 'np',\n",
       " 'one_hot',\n",
       " 'pad',\n",
       " 'pairwise_distance',\n",
       " 'pdist',\n",
       " 'pixel_shuffle',\n",
       " 'pixel_unshuffle',\n",
       " 'poisson_nll_loss',\n",
       " 'prelu',\n",
       " 'relu',\n",
       " 'relu6',\n",
       " 'relu_',\n",
       " 'reproducibility_notes',\n",
       " 'rrelu',\n",
       " 'rrelu_',\n",
       " 'scaled_dot_product_attention',\n",
       " 'selu',\n",
       " 'selu_',\n",
       " 'sigmoid',\n",
       " 'silu',\n",
       " 'smooth_l1_loss',\n",
       " 'soft_margin_loss',\n",
       " 'softmax',\n",
       " 'softmin',\n",
       " 'softplus',\n",
       " 'softshrink',\n",
       " 'softsign',\n",
       " 'sparse_support_notes',\n",
       " 'tanh',\n",
       " 'tanhshrink',\n",
       " 'tf32_notes',\n",
       " 'threshold',\n",
       " 'threshold_',\n",
       " 'torch',\n",
       " 'triplet_margin_loss',\n",
       " 'triplet_margin_with_distance_loss',\n",
       " 'unfold',\n",
       " 'upsample',\n",
       " 'upsample_bilinear',\n",
       " 'upsample_nearest',\n",
       " 'utils',\n",
       " 'warnings']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading",
   "language": "python",
   "name": "trading"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
